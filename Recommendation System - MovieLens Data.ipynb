{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System - Movielens Data\n",
    "\n",
    "Aim - Build a recommendation engine using Movielens data to predict the movies a user would enjoy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark MLlib library for Machine Learning provides a Collaborative Filtering implementation by using Alternating Least Squares. The implementation in MLlib has these parameters:\n",
    "\n",
    "* numBlocks is the number of blocks used to parallelize computation (set to -1 to auto-configure).\n",
    "* rank is the number of latent factors in the model.\n",
    "* iterations is the number of iterations to run.\n",
    "* lambda specifies the regularization parameter in ALS.\n",
    "* implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data.\n",
    "* alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create a spark session and import data\n",
    "2. Check whether data is in movieID, userID and rating (all numerical) format\n",
    "3. Check for missing values and see the summary to get an idea of the data\n",
    "4. Split data into train and test to evaluate the performance of the recommendation system\n",
    "5. Import ALS and build the model on the train set\n",
    "6. Predict the ratings on the test set\n",
    "7. Import an evaluator to evaluate the accuracy of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('movie').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import movielens data\n",
    "\n",
    "data = spark.read.csv('movielens_ratings.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movieId': 2, 'rating': 3.0, 'userId': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head().asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|           movieId|            rating|            userId|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|              1501|              1501|              1501|\n",
      "|   mean| 49.40572951365756|1.7741505662891406|14.383744170552964|\n",
      "| stddev|28.937034065088994| 1.187276166124803| 8.591040424293272|\n",
      "|    min|                 0|               1.0|                 0|\n",
      "|    max|                99|               5.0|                29|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|movieId|rating|userId|\n",
      "+-------+------+------+\n",
      "|      0|     0|     0|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for any missing values\n",
    "from pyspark.sql.functions import isnan, isnull, when, count, col\n",
    "\n",
    "data.select([count(when(isnan(c)| isnull(c), c)).alias(c) for c in data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test set to evaluate the performance of our recommendation system\n",
    "train, test = data.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ALS\n",
    "\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
    "model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|movieId|rating|userId|prediction|\n",
      "+-------+------+------+----------+\n",
      "|     31|   1.0|    27|0.42751443|\n",
      "|     31|   1.0|     5| 2.7613196|\n",
      "|     78|   1.0|     1|0.96409684|\n",
      "|     34|   1.0|     4| 1.2006195|\n",
      "|     81|   1.0|     7| 0.7912039|\n",
      "|     28|   1.0|     2| 4.9103336|\n",
      "|     26|   1.0|    19| 0.8854952|\n",
      "|     26|   3.0|    15|  2.203689|\n",
      "|     26|   1.0|     2| 2.0831113|\n",
      "|     27|   1.0|     5|  4.838405|\n",
      "|     27|   3.0|    24| 0.9148974|\n",
      "|     44|   1.0|    22|  0.672422|\n",
      "|     44|   4.0|    18| 1.0771607|\n",
      "|     12|   1.0|     1| 1.3843888|\n",
      "|     12|   1.0|    21|0.63092315|\n",
      "|     12|   2.0|    18| 0.4667408|\n",
      "|     91|   1.0|    20| 1.2612588|\n",
      "|     47|   1.0|    26|-1.6067481|\n",
      "|     47|   4.0|    25|  2.358118|\n",
      "|     47|   1.0|    24| 2.4211466|\n",
      "+-------+------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluator\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our predicted ratings have a continuous value, we decided to use the Regression Evaluator to evaluate our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error = 1.5892349847085798\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root mean square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty bad since from a rating of 1 to 5 stars the error is nearly approximately of 2 stars. We can attribute the high value of RMSE to the small dataset (only 1501 observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will use the model to predict the ratings of a single user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_user = test.filter(test['userId']==11).select(['movieId','userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|movieId|userId|\n",
      "+-------+------+\n",
      "|     10|    11|\n",
      "|     20|    11|\n",
      "|     36|    11|\n",
      "|     39|    11|\n",
      "|     62|    11|\n",
      "|     75|    11|\n",
      "|     82|    11|\n",
      "|     88|    11|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check which movies userId '11' has seen\n",
    "single_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = model.transform(single_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|movieId|userId|prediction|\n",
      "+-------+------+----------+\n",
      "|     10|    11|  2.567011|\n",
      "|     39|    11| 2.2638342|\n",
      "|     20|    11| 2.1479535|\n",
      "|     36|    11| 1.9692445|\n",
      "|     88|    11| 1.9242296|\n",
      "|     82|    11| 1.7972623|\n",
      "|     75|    11| 1.5097756|\n",
      "|     62|    11|-2.6585352|\n",
      "+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendations.orderBy('prediction',ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
