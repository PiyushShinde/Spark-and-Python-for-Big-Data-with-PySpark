{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering - Technology Firm's Hacked Data\n",
    "\n",
    "Aim - Use hacked data assembled by forensic engineers of a Technology Firm to identify the number of hackers that perpetrated the attack.\n",
    "\n",
    "The features of the data are as follows:\n",
    "\n",
    "* 'Session_Connection_Time': How long the session lasted in minutes\n",
    "* 'Bytes Transferred': Number of MB transferred during session\n",
    "* 'Kali_Trace_Used': Indicates if the hacker was using Kali Linux\n",
    "* 'Servers_Corrupted': Number of server corrupted during the attack\n",
    "* 'Pages_Corrupted': Number of pages illegally accessed\n",
    "* 'Location': Location attack came from (Probably useless because the hackers used VPNs)\n",
    "* 'WPM_Typing_Speed': Their estimated typing speed based on session logs.\n",
    "\n",
    "One key evidence - The forensic engineer knows that the hackers trade off attacks. Meaning they should each have roughly the same amount of attacks. For example if there were 100 total attacks, then in a 2 hacker situation each should have about 50 hacks, in a three hacker situation each would have about 33 hacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to follow: \n",
    "\n",
    "1. Create a Spark Session and load data\n",
    "2. Check for missing values (if yes, drop or fill them)\n",
    "3. Check whether or not data is in the format - features (since it's unsupervised learning we do not need labels) \n",
    "4. If not in 'features' format, assemble the features using an assembler.\n",
    "5. Scale the features to avoid curse of high dimensionality (even if there aren't many features it's a good practise to employ scaling of features in unsupervised learning)\n",
    "6. Import Kmeans and create it's instance\n",
    "5. Create multiple models of different number of clusters and employ elbow method to identify a drop in WSSE for a particular number of custers\n",
    "6. If this method fails to provide any evidence on the number of hackers then check the number of attacks of each hacker for multiple number of clusters. If for a particular number of clusters, the number of attacks by hackers are equal, we have the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('cluster').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data\n",
    "data = spark.read.csv(\"hack_data.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bytes Transferred': 391.09,\n",
       " 'Kali_Trace_Used': 1,\n",
       " 'Location': 'Slovenia',\n",
       " 'Pages_Corrupted': 7.0,\n",
       " 'Servers_Corrupted': 2.96,\n",
       " 'Session_Connection_Time': 8.0,\n",
       " 'WPM_Typing_Speed': 72.37}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head().asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All numerical features except for location, but the hackers would probably be using VPN so we could drop this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------+----------------+\n",
      "|Session_Connection_Time|Bytes Transferred|Kali_Trace_Used|Servers_Corrupted|Pages_Corrupted|Location|WPM_Typing_Speed|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------+----------------+\n",
      "|                      0|                0|              0|                0|              0|       0|               0|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for any missing values\n",
    "from pyspark.sql.functions import isnan, isnull, when, count, col\n",
    "\n",
    "data.select([count(when(isnan(c)| isnull(c), c)).alias(c) for c in data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "|summary|Session_Connection_Time| Bytes Transferred|   Kali_Trace_Used|Servers_Corrupted|   Pages_Corrupted|   Location|  WPM_Typing_Speed|\n",
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "|  count|                    334|               334|               334|              334|               334|        334|               334|\n",
      "|   mean|     30.008982035928145| 607.2452694610777|0.5119760479041916|5.258502994011977|10.838323353293413|       null|57.342395209580864|\n",
      "| stddev|     14.088200614636158|286.33593163576757|0.5006065264451406| 2.30190693339697|  3.06352633036022|       null| 13.41106336843464|\n",
      "|    min|                    1.0|              10.0|                 0|              1.0|               6.0|Afghanistan|              40.0|\n",
      "|    max|                   60.0|            1330.5|                 1|             10.0|              15.0|   Zimbabwe|              75.0|\n",
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a summary of the data\n",
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Session_Connection_Time',\n",
       " 'Bytes Transferred',\n",
       " 'Kali_Trace_Used',\n",
       " 'Servers_Corrupted',\n",
       " 'Pages_Corrupted',\n",
       " 'Location',\n",
       " 'WPM_Typing_Speed']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check columns\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Session_Connection_Time', 'Bytes Transferred', 'Kali_Trace_Used',\n",
    "             'Servers_Corrupted', 'Pages_Corrupted','WPM_Typing_Speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = cols, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_Connection_Time: double (nullable = true)\n",
      " |-- Bytes Transferred: double (nullable = true)\n",
      " |-- Kali_Trace_Used: integer (nullable = true)\n",
      " |-- Servers_Corrupted: double (nullable = true)\n",
      " |-- Pages_Corrupted: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- WPM_Typing_Speed: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always better to scale the features to avoid curse of dimensionality\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale with respect to standard deviation\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", \n",
    "                        outputCol=\"scaledFeatures\", \n",
    "                        withStd=True, \n",
    "                        withMean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each feature to have unit standard deviation.\n",
    "cluster_final_data = scalerModel.transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_Connection_Time: double (nullable = true)\n",
      " |-- Bytes Transferred: double (nullable = true)\n",
      " |-- Kali_Trace_Used: integer (nullable = true)\n",
      " |-- Servers_Corrupted: double (nullable = true)\n",
      " |-- Pages_Corrupted: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- WPM_Typing_Speed: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaledFeatures: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure it works\n",
    "cluster_final_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether it was 2 or 3 hackers\n",
    "\n",
    "# For 3 hackers\n",
    "kmeans3 = KMeans(featuresCol='scaledFeatures',k=3)\n",
    "model_k3 = kmeans3.fit(cluster_final_data)\n",
    "wssse_k3 = model_k3.computeCost(cluster_final_data)\n",
    "\n",
    "# For 2 hackers\n",
    "kmeans2 = KMeans(featuresCol='scaledFeatures',k=2)\n",
    "model_k2 = kmeans2.fit(cluster_final_data)\n",
    "wssse_k2 = model_k2.computeCost(cluster_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 3 hackers: \n",
      "Within Set Sum of Squared Errors = 434.1492898715845\n",
      "------------------------------------------------------------\n",
      "For 2 hackers: \n",
      "Within Set Sum of Squared Errors = 601.7707512676716\n"
     ]
    }
   ],
   "source": [
    "print(\"For 3 hackers: \")\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse_k3))\n",
    "print('--'*30)\n",
    "print(\"For 2 hackers: \")\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse_k2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't enough evidence that confirms whether they were 2 or 3 hackers. And as we know in Kmeans as number of clusters increase the WSSE decreases. We could check the WSSE for more number of clusters to check whether the WSSE suddenly drops for a particular number of clusters (Elbow Method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 2 hackers: \n",
      "Within Set Sum of Squared Errors = 601.7707512676716\n",
      "------------------------------------------------------------\n",
      "For 3 hackers: \n",
      "Within Set Sum of Squared Errors = 434.1492898715845\n",
      "------------------------------------------------------------\n",
      "For 4 hackers: \n",
      "Within Set Sum of Squared Errors = 267.1336116887891\n",
      "------------------------------------------------------------\n",
      "For 5 hackers: \n",
      "Within Set Sum of Squared Errors = 401.0534138416211\n",
      "------------------------------------------------------------\n",
      "For 6 hackers: \n",
      "Within Set Sum of Squared Errors = 240.58180197600024\n",
      "------------------------------------------------------------\n",
      "For 7 hackers: \n",
      "Within Set Sum of Squared Errors = 224.75676930192807\n",
      "------------------------------------------------------------\n",
      "For 8 hackers: \n",
      "Within Set Sum of Squared Errors = 228.32460058156488\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for k in range(2,9):\n",
    "    kmeans = KMeans(featuresCol='scaledFeatures',k=k)\n",
    "    model = kmeans.fit(cluster_final_data)\n",
    "    wssse = model.computeCost(cluster_final_data)\n",
    "    print(\"For {} hackers: \".format(k))\n",
    "    print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "    print('--'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no sudden drop in WSSE for any of the above considered number of clusters. \n",
    "\n",
    "Lastly we could check the number of trade off attacks by the hackers since as mentioned by the forensic engineer the hackers had roughly same amount of attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  167|\n",
      "|         2|   83|\n",
      "|         0|   84|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check number of attacks for 3 hackers\n",
    "\n",
    "model_k3.transform(cluster_final_data).groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  167|\n",
      "|         0|  167|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check number of attacks for 2 hackers\n",
    "\n",
    "model_k2.transform(cluster_final_data).groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, there is a even split in the number of attacks for 2 hackers. This proves that 2 hackers perpetrated the attack on the technology firm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
